#Data pre-processing
Standardization makes features in the original feature space be compatible with
each other with regards to the measurement scale. This is important in many Data
Mining and Machine Learning analyses, and especially for the PCA, which aims 
to preserve variance. If there is significant difference in measurement scales
between features (e.g., one feature is measured in mm and all others in m), 
the transformation will mainly pick up on the variance produced by some of the 
features and miss out of the more minute differences in the others.

#DATA PREPROCESSING

          import pandas as pd
          import numpy as np
          from sklearn.preprocessing import StandardScaler

          # read in the dataset
          df = pd.read_csv(
              filepath_or_buffer='data/wine.data', 
              header=None, 
              sep=',')


          # extract the vectors from the Pandas data file
          X = df.iloc[:,1:].values
          print(df.shape)
          # standardise the data
          X_std = StandardScaler().fit_transform(X)
          
# subtract the mean vector from each vector in the dataset
           means = np.mean(X_std, axis=0)
           X_sm = X_std - means
           
           
#Algorithm 1: Computing PCA via the covariance matrix

          X_cov = X_sm.T.dot(X_sm) / (X_sm.shape[0] - 1)

# Side-note: Numpy has a function for computing the covariance matrix
          X_cov2 = np.cov(X_std.T)
          print("X_cov == X_cov2: ", np.allclose(X_cov, X_cov2))

          # perform the eigendecomposition of the covariance matrix
          eig_vals, eig_vecs = np.linalg.eig(X_cov)
          print('Eigen Vec',eig_vecs)
          
#What remains now is to pick the eigenvectors (columns in eig_vecs) 
associated with the eigenvalues in eig_vals with the highest absolute values. 
Let's see first the percent variance expained by each eigenvalue-eigenvector pair. 
To do this, we sort the absolute eigenvalues and transform the values into percentages
by performing L-1 normalization. We then perform a prefix-sum operation on the vector
of percentages. The resulting vector will show us, in its  jj th dimension, the percent
of explained variance in the PCA dimensionality reduction using  jj  dimensions. We will 
create a function that we can reuse to do this transformation.


          def percvar(v):
              r"""Transform eigen/singular values into percents.
              Return: vector of percents, prefix vector of percents
              """
              # sort values
              s = np.sort(np.abs(v))
              # reverse sorting order
              s = s[::-1]
              # normalize
              s = s/np.sum(s)
              return s, np.cumsum(s)
          print("eigenvalues:    ", eig_vals)
          pct, pv = percvar(eig_vals)
          print("percent values: ", pct)
          print("prefix vector:  ", pv)
          
          
#Ploting the pct and pv vectors and observe the general
trend of the variance as more and more dimensions are added.


# plot feature and overall percent variance
          %matplotlib inline
          import matplotlib.pyplot as plt
          plt.plot(range(1,len(pct)+1),pct,label = "fetaure")
          plt.plot(range(1,len(pv)+1),pv,label = "Overall")
          
          
#Creating a function that, given the overall percent varience vector 
plotted in the previous exercise and the expected percent variance  pp ,
returns the number of latent space dimensions that account for  pp % variance 
in the data. Print out the number of dimensions for  p∈{40,60,80,90,95}p∈{40,60,80,90,95} .


          def perck(s, p):
              for i in range(len(s)):
                  if(s[i]>=p):
                      return i+1
              return len(s)
          
          for p in [40, 60, 80, 90, 95]:
              print("Number of dimensions to account for %d%% of the variance: %d" % (p, perck(pv, p*0.01)))
              
#Algorithm 2: Computing PCA via the Singular Value Decomposition (SVD)¶

We can instead compute the PCA trasformation via the SVD of the centered 
matrix  X=XsmX=Xsm . However, we will then need to transform the singular values 
of  XX into eigenvalues of  XTXXTX  before constructing the percent vector. 
In general, the non-zero singular values of a matrix  XX  are the square roots 
of the eigenvalues of the square matrix  XTXXTX .


          U,s,V = np.linalg.svd(X_sm)
          # singular values of X are the square roots of the eigenvalues of the square matrix X^TX
          print("singular values:        ", s)
          print("eigenvalues:            ", (np.sort(np.abs(eig_vals)))[::-1])
          print("scaled singular values: ", (s**2/(X_sm.shape[0]-1)))
