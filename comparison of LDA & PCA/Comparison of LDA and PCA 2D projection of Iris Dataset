#Principal Component Analysis (PCA) applied to this data identifies the combination of attributes
(principal components, or directions in the feature space) that account for the most variance in the data. 
Here we plot the different samples on the 2 first principal components.

Linear Discriminant Analysis (LDA) tries to identify attributes that account for the most variance between classes.
In particular, LDA, in contrast to PCA, is a supervised method, using known class labels.


#Importing required Libraries

      import matplotlib.pyplot as plt

      from sklearn import datasets
      from sklearn.decomposition import PCA
      from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

      iris = datasets.load_iris()

      X = iris.data
      y = iris.target
      target_names = iris.target_names
      target_names
   
      pca = PCA(n_components=2)
      X_r = pca.fit(X).transform(X)

      lda = LinearDiscriminantAnalysis(n_components=2)
      X_r2 = lda.fit(X, y).transform(X)
      
#Percentage of variance explained for each components

      print('explained variance ratio (first two components): %s'
            % str(pca.explained_variance_ratio_))

      plt.figure()
      colors = ['navy', 'turquoise', 'darkorange']
      linewidth = 2
      
      
      
#Scatter Plots for PCA and LDA

      for color, i, target_name in zip(colors, [0, 1, 2], target_names):
          plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=.8, lw=linewidth,
                      label=target_name)
      plt.legend(loc='best', shadow=False, scatterpoints=1)
      plt.title('PCA of IRIS dataset')

      plt.figure()
      for color, i, target_name in zip(colors, [0, 1, 2], target_names):
          plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], alpha=.8, color=color,
                      label=target_name)
      plt.legend(loc='best', shadow=False, scatterpoints=1)
      plt.title('LDA of IRIS dataset')

      plt.show()
      
      
      
      
      
      
#Compare of LDA and PCA 2D projection for Digits dataset


digits = datasets.load_digits()

3Each datapoint is a 8x8 image of a hand-written digit. No. of 
Classes 10 Samples per class ~180 Samples total 1797 Dimensionality 64 Features integers 0-16


        x = digits.data
        y = digits.target
        target_names = digits.target_names

        pca = PCA(n_components=2)
        X_r = pca.fit(x).transform(x)
        lda = LinearDiscriminantAnalysis(n_components=2)
        X_r2 = lda.fit(x, y).transform(x)
        print('explained variance ratio (first two components): %s'
              % str(pca.explained_variance_ratio_))

        plt.figure()
        colors = ['red', 'green', 'blue', 'purple', 'navy', 'turquoise', 'darkorange', 'pink', 'yellow', 'orange']
        linewidth = 2
        r = [i for i in range(0, len(target_names))]
        for color, i, target_name in zip(colors,r , target_names):
            plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=.8, lw=linewidth,
                        label=target_name)
        plt.legend(loc='best', shadow=False, scatterpoints=1)
        plt.title('PCA of Digits dataset')

        plt.figure()
        for color, i, target_name in zip(colors, r, target_names):
            plt.scatter(X_r2[y == i, 0], X_r2[y == i, 1], alpha=.8, color=color,
                        label=target_name)
        plt.legend(loc='best', shadow=False, scatterpoints=1)
        plt.title('LDA of Digits dataset')
        plt.show()
